---
title: "global_optimization"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{global_optimization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup}
library(soundlocalization)

library(lubridate)
library(tidyverse)
```



Lets explore how to chop the recordings into a series of frames where the gcc-phase analysis is done separately. 

So, all the different algorithms (fft, gcc-phase. gcc-phase-common-peaks) are applied to each frame. 

First, we load a set of concurrent recordings

```{r}



files <- list(
  system.file("extdata", "Grabacion_2_andrea.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_1_joaco.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_1_cande.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_3_luciano.wav", package = "soundlocalization")
)

files_and <- list(
  system.file("extdata", "Grabacion_2_andrea.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_2_andrea.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_2_andrea.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_2_andrea.wav", package = "soundlocalization")
)

labels = c("Andrea", "Joaco", "Candela", "Luciano")
geo_locations = rbind(
  geo_coordinates(
    lat = -34.61410605441556,
    lon = -58.38404404725583
  ),
  geo_coordinates(
    lat = -34.614103295104634,
    lon = -58.3839950969408
  ),
  geo_coordinates(
    lat = -34.6140999839314,
    lon = -58.383942123312224
  ),
  
  geo_coordinates(
    lat = -34.61409722462029,
    lon = -58.38389920796756
  )
)
time_of_recording = lubridate::parse_date_time("2022-09-03 10:52",orders = "ymd HM", tz = "America/Buenos_Aires")

rec <- recordings(
  files = files,
  labels = labels,
  geo_locations = geo_locations,
  time_of_recording = time_of_recording, 
  velocity_of_sound = 334
)

rec_fake <- recordings(
  files = files_and,
  labels = labels,
  geo_locations = geo_locations,
  time_of_recording = time_of_recording, 
  velocity_of_sound = 334, 
  position_x = c(0,0,1,-1),
  position_y = c(1,-1,0, 0)
)

```


```{r}
a_fake <- align_recordings(rec_fake)
```


```{r}
f_fake <- frame_recordings(a_fake, i_start_diff = 120*rec$fs, nsamples = 2048, window_size_in_seconds = 1024/a_fake$fs)
```


```{r}
model=init_model(f_fake,n_sources = 2, include_frame_gain = T, freq_limits = rbind(c(1,5000)))
opt_fake = global_optimization(f_fake,model)

```



then we need to align the recordings and define frames


```{r}
a_rec <- align_recordings(rec)
a2_rec <- align_recordings(a_rec)
```


```{r}
f_rec <- frame_recordings(a2_rec, i_start_diff = 120*rec$fs, nsamples = rec$fs)
```



```{r}
init_parameters(f_rec) -> f_rec
parameters_to_beta(f_rec) -> bb
beta_to_parameters(bb,f_rec) ->ff
all.equal(f_rec,ff)

```


```{r}
xdata= parameters_to_xdata(f_rec)
get_predicted_signal(f_rec,xdata) -> Yfit
sqr_sum_signal(f_rec,bb,xdata) -> ss
ss
```

```{r}
model = init_model(f_fake, include_frame_gain = F)
opt_fake = global_optimization(f_fake, model)
```

