---
title: "soundlocalization"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{soundlocalization}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(soundlocalization)
library(ggplot2)
library(lubridate)
```

The purpose of this package is to provide tools for isolating and
localizing the source of sounds using an array of receptors on a not particularly regular order.

We define as a recording session, a list of recordings taken approximately simultaneously on an array of receptors. 

For each session we need an identifying arbitrary label for the session, the approximate time of the start of the recordings and a list of recordings each with a unique identifying label, the file name of the audio archive and the approximate location of the receptor (latitude and longitude and height above the ground).  



```{r}
files <- list(
  system.file("extdata", "/campo/Grabacion 10_andrea.wav", package = "soundlocalization"),
  system.file("extdata", "/campo/Grabacion 6_candela.wav", package = "soundlocalization"),
  system.file("extdata", "/campo/Grabacion 3_joaquin.wav", package = "soundlocalization"),
  system.file("extdata", "/campo/Voz 006_julieta.wav", package = "soundlocalization"),
  system.file("extdata", "/campo/Grabacion 18_luciano.wav", package = "soundlocalization")
)
labels = c("Andrea", "Candela", "Joaco", "Julieta","Luciano")
lat= c(-35.100313,-35.099890,-35.1003703,-35.099870,-35.100110)
lon= c(-58.304676,-58.304550,-58.3041342,-58.304039,-58.304306)
elevations =c(1.4,1.42,1.48,1.4,1.54)
time_of_recording = lubridate::ymd_hm("20221225 1735")
```


```{r}
session <- build_session(label = "detras_de_la_casa",receptors = labels,latitudes = lat,longitudes = lon,elevations = elevations,time_of_recording = time_of_recording, files = files)

```

Lets see the organization of rec

```{r}
str(rec)
```

Now the idea of soundlocalization is to locate the time, frequency and point of origin of the sounds of a recording. 

There are two elements in soundlocalization:

1. Sound receptors
2. Sound sources (or emitters)

We have an initial measure of the position of the receptors and possibly of some sources. 

We have the data of the time of each of the simultaneous recordings done on each receptor. 

With purpose of a clear analysis of the recordings it is convenient to define frames as intervals of time and frequency. 

The main tool for the separation of the sound sources is the generalized cross correlation -phase, a normalization of the cross correlation function. The idea is that separate sources would generate separate peaks in the cross correlation between the receptor pairs. 
The possition of the peaks is given by the time difference in the arrival of the sound to different receptors, therefore by analyzing all the possible pairs, it is possible to assign the peaks of the pair A-B to the peaks in B-C and A-C. 
One of the algorithms of soundlocalization identifies the shared peaks that arrive at given times at all the receptors. 
A second algorithm assign the shared peaks to source positions. However those positions depend on a proper sincronization of the recordings, something that is usually not feasible, so we need a third algorithm to sinchronize the recordings, that is determine the relative offset of each receptor. This is possible if we know with precision the position of one or more sound sources or if we have a good number of them. 

The whole idea of soundlocalization consist in optimize everything at the same time: the offset of each receptor, the precise relative position of each receptor and sound source and the gain of each receptor at each time (sometimes the recordings have automatic gain adjustments)

For doing that it is necessary to do three things: 
1. Indicate the possible position of the receptors with some error estimates
2. indicate the possible time, frequency and position coordinates of some sounds sources.
3. Organize the recording on different frames defined by time and frequency intervals (might be also regions of spaceÂ¿?).


# Indicate the position of receptors with their errors



The idea is to use geocoordinates for the position (including altitude for height) and UTC for time of the recording. The frequecy is simpler to understand. 




Now we define a list of some identified sound sources


```{r}
sources = set_sources(
  label = c(
    'Andrea_voice',
    'Joaquin_voice',
    'Candela_voice',
    'Luciano_voice'
  ),
  x_pos = c(0, 4.5, 9.3, 13.3),
  y_pos = c(0.1, 0.1, 0.1, 0.1),
  error = 2,
  t_recordings = c('Andrea',
                   'Joaquin',
                   'Candela',
                   'Luciano'),
  t_starts = c(123.221, 121.876, 122.556,120.460),
  t_ends = c(124.109, 122.433,123.249,122.969),
  min_freq = 500,
  max_freq = 6000)
sources
```

Now, with the information on the sound sources, we can organize the analysis of the recordings in different frames. Each frame has a start and end time (according to a given recording)






