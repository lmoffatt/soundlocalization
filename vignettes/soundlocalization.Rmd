---
title: "soundlocalization"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{soundlocalization}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(soundlocalization)
library(ggplot2)
library(lubridate)
```

The purpose of this package is to provide tools for isolating and
localizing the source of sounds from an array of receptors, not
necessary arranged in a regular order.

Lets do by example. Here is a list of recordings taken on a line array
of approx 4 m between receptors.

```{r}
files <- list(
  system.file("extdata", "Grabacion_2_andrea.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_1_joaco.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_1_cande.wav", package = "soundlocalization"),
  system.file("extdata", "Grabacion_3_luciano.wav", package = "soundlocalization")
)
labels = c("Andrea", "Joaco", "Candela", "Luciano")
geo_locations = rbind(
  geo_coordinates(
    lat = -34.61410605441556,
    lon = -58.38404404725583
  ),
  geo_coordinates(
    lat = -34.614103295104634,
    lon = -58.3839950969408
  ),
  geo_coordinates(
    lat = -34.6140999839314,
    lon = -58.383942123312224
  ),
  
  geo_coordinates(
    lat = -34.61409722462029,
    lon = -58.38389920796756
  )
)
time_of_recording = ymd_hm("2022-09-03 10:52", tz = "America/Buenos_Aires")
```

The receptors were placed in the same line (all the xs were equal) at
distances of 5, 4 and 4 meters.

We use the function recordings to organize all the recordings at once.

```{r}
rec<-recordings(files = files,
                       labels = labels,
                       geo_locations = geo_locations, 
                       time_of_recording = time_of_recording)

```

Lets see the organization of rec

```{r}
str(rec)
```

Now the idea of soundlocalization is to locate the time, frequency and point of origin of the sounds of a recording. 

There are two elements in soundlocalization:

1. Sound receptors
2. Sound sources (or emitters)

We have an initial measure of the position of the receptors and possibly of some sources. 

We have the data of the time of each of the simultaneous recordings done on each receptor. 

With purpose of a clear analysis of the recordings it is convenient to define frames as intervals of time and frequency. 

The main tool for the separation of the sound sources is the generalized cross correlation -phase, a normalization of the cross correlation function. The idea is that separate sources would generate separate peaks in the cross correlation between the receptor pairs. 
The possition of the peaks is given by the time difference in the arrival of the sound to different receptors, therefore by analyzing all the possible pairs, it is possible to assign the peaks of the pair A-B to the peaks in B-C and A-C. 
One of the algorithms of soundlocalization identifies the shared peaks that arrive at given times at all the receptors. 
A second algorithm assign the shared peaks to source positions. However those positions depend on a proper sincronization of the recordings, something that is usually not feasible, so we need a third algorithm to sinchronize the recordings, that is determine the relative offset of each receptor. This is possible if we know with precision the position of one or more sound sources or if we have a good number of them. 

The whole idea of soundlocalization consist in optimize everything at the same time: the offset of each receptor, the precise relative position of each receptor and sound source and the gain of each receptor at each time (sometimes the recordings have automatic gain adjustments)

For doing that it is necessary to do three things: 
1. Indicate the possible position of the receptors with some error estimates
2. indicate the possible time, frequency and position coordinates of some sounds sources.
3. Organize the recording on different frames defined by time and frequency intervals (might be also regions of spaceÂ¿?).


# Indicate the position of receptors with their errors



The idea is to use geocoordinates for the position (including altitude for height) and UTC for time of the recording. The frequecy is simpler to understand. 




Now we define a list of some identified sound sources


```{r}
sources = set_sources(
  label = c(
    'Andrea_voice',
    'Joaquin_voice',
    'Candela_voice',
    'Luciano_voice'
  ),
  x_pos = c(0, 4.5, 9.3, 13.3),
  y_pos = c(0.1, 0.1, 0.1, 0.1),
  error = 2,
  t_recordings = c('Andrea',
                   'Joaquin',
                   'Candela',
                   'Luciano'),
  t_starts = c(123.221, 121.876, 122.556,120.460),
  t_ends = c(124.109, 122.433,123.249,122.969),
  min_freq = 500,
  max_freq = 6000)
sources
```

Now, with the information on the sound sources, we can organize the analysis of the recordings in different frames. Each frame has a start and end time (according to a given recording)






